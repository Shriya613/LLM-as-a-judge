{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Combined Judgements\n",
    "\n",
    "The purpose of this file is just to combine the LM Judge judgement file with the Human Annotation judgement files.\n",
    "\n",
    "Note that the human annotations only cover 20 out of the 50 questions for each dataset.\n",
    "So we should probably filter the LM Judge datasets to the same questions before combining the two, since it wouldn't make sense to compare LMJ and Humans on different distributions of data.\n",
    "\n",
    "\n",
    "I could see arguments that this or its outputs shouldn't be in the /judgements directory. It's a little messy, I admit ðŸ˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jdf = pd.read_csv(\"judgements_eli5_HQ_20250311_213806.csv\") # (LM) \"Judge df\"\n",
    "# hdf = pd.read_csv(\"../annotation/HumanAnnotationQuestionAnswering_corrected.csv\")  # \"Human df\"\n",
    "jdf = pd.read_csv(\"judgements_writing_prompts_HQ_20250312_113801.csv\")\n",
    "hdf = pd.read_csv(\"../annotation/HumanAnnotationCreativeWriting_corrected.csv\")  # Making sure to use CORRECTED versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of jdf: 24300\n",
      "Length of hdf: 1080\n",
      "Column match: [ True  True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "Question ID ranges:\n",
      "LM Judge df: 0 - 49\n",
      "Human df: 0 - 19\n",
      "\n",
      "Judgement ID counts in LM Judge df:\n",
      "6    4050\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Judgement ID counts in Human df:\n",
      "6    180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Judgement criteria ID uniqueness check:\n",
      "LM Judge df unique criteria IDs: 24300 ... Good? True\n",
      "Human df unique criteria IDs: 1080 ... Good? True\n",
      "\n",
      "Criteria comparison:\n",
      "LM Judge criteria: {'Flow', 'Style', 'Emotion', 'Plot', 'Dialogue', 'Character'}\n",
      "Human criteria: {'Flow', 'Style', 'Plot', 'Emotion', 'Dialogue', 'Character'}\n",
      "Criteria match: True\n"
     ]
    }
   ],
   "source": [
    "# INTEGRITY CHECKS\n",
    "\n",
    "print(f\"Length of jdf: {len(jdf)}\")\n",
    "print(f\"Length of hdf: {len(hdf)}\")\n",
    "\n",
    "# Do the columns match?\n",
    "print(f\"Column match: {jdf.columns == hdf.columns}\")\n",
    "\n",
    "# What's the range of questions for each?\n",
    "print(\"\\nQuestion ID ranges:\")\n",
    "print(f\"LM Judge df: {jdf['question_id'].min()} - {jdf['question_id'].max()}\")\n",
    "print(f\"Human df: {hdf['question_id'].min()} - {hdf['question_id'].max()}\")\n",
    "\n",
    "# Are there any judgement_ids that appear anything other than 6 times, in either?\n",
    "print(\"\\nJudgement ID counts in LM Judge df:\")\n",
    "print(jdf.groupby('judgement_id').size().value_counts())\n",
    "print(\"\\nJudgement ID counts in Human df:\") \n",
    "print(hdf.groupby('judgement_id').size().value_counts())\n",
    "\n",
    "# If every judgement_crtieria_id unique?\n",
    "print(\"\\nJudgement criteria ID uniqueness check:\")\n",
    "print(f\"LM Judge df unique criteria IDs: {len(jdf['judgement_criteria_id'].unique())} ... Good? {len(jdf['judgement_criteria_id'].unique()) == len(jdf)}\")\n",
    "print(f\"Human df unique criteria IDs: {len(hdf['judgement_criteria_id'].unique())} ... Good? {len(hdf['judgement_criteria_id'].unique()) == len(hdf)}\")\n",
    "\n",
    "# Do both have the same criteria?\n",
    "# Do both have the same criteria?\n",
    "print(\"\\nCriteria comparison:\")\n",
    "jdf_criteria = set(jdf['criteria'].unique())\n",
    "hdf_criteria = set(hdf['criteria'].unique())\n",
    "print(f\"LM Judge criteria: {jdf_criteria}\")\n",
    "print(f\"Human criteria: {hdf_criteria}\")\n",
    "print(f\"Criteria match: {jdf_criteria == hdf_criteria}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of new_df: 25380\n",
      "Length of new_df: 10800\n",
      "\n",
      "Question ID range in filtered df:\n",
      "new_df: 0 - 19\n"
     ]
    }
   ],
   "source": [
    "# Now let's combine the dataframes\n",
    "new_df = pd.concat([jdf, hdf], ignore_index=True)\n",
    "\n",
    "print(f\"Length of new_df: {len(new_df)}\")\n",
    "\n",
    "\n",
    "# And filter to only include questions that Humans have annotated   \n",
    "new_df = new_df[new_df['question_id'].isin(hdf['question_id'])] \n",
    "\n",
    "print(f\"Length of new_df: {len(new_df)}\")\n",
    "\n",
    "# What's the range of questions in the filtered df?\n",
    "print(\"\\nQuestion ID range in filtered df:\")\n",
    "print(f\"new_df: {new_df['question_id'].min()} - {new_df['question_id'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's save the new dataframe\n",
    "new_df.to_csv(\"judgements_writing_prompts_combined_with_humans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
